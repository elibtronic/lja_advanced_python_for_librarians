{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMozOlmgVlHJRxCn/HOYLla"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 4 Workalong\n"
      ],
      "metadata": {
        "id": "wKjXWA_Zn4T4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This week we are going to look at running a local LLM in the Colab environment. What we are going to do is a bit similar to what happens when we you use ChatGPT in your browser. Similar, not quite the same. We'll use different models downloaded from [HuggingFace]() which is a repository of LLMs an similar tools. We'll use the library [Llama cpp]() to interact with the model.\n",
        "\n",
        "For homework this week I'll ask you to pick a model and recreate a lot of what we are doing here as a way of testing the capabilities of different LLMs.\n",
        "\n",
        "We will need to be patient with this week's notebooks. The files we are working with are pretty large and downloading them and running them will require more time than we've needed seen before\n",
        "\n",
        "\n",
        "The impact LLMs are having our how we do work is still being understood. Take this study from [Microsoft](https://www.404media.co/microsoft-study-finds-ai-makes-human-cognition-atrophied-and-unprepared-3/)...\n",
        "\n",
        "\n",
        "https://colab.research.google.com/github/R3gm/InsightSolver-Colab/blob/main/LLM_Inference_with_llama_cpp_python__Llama_2_13b_chat.ipynb#scrollTo=R76uxL293jTc"
      ],
      "metadata": {
        "id": "4TINJCqCoV1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Switch to GPU Runtime\n",
        "\n",
        "Since Google Colab gives us free access to a notebook with GPU power let's switch to that instead.\n",
        "1. Click the dropdown arrow underneath your picture\n",
        "1. _Change Runtime Type_\n",
        "1. _T4 GPU_"
      ],
      "metadata": {
        "id": "8pCBR4uhgb5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Installing Extra Libraries\n",
        "\n",
        "Like our Week 4 Warm Up activity showed use, sometimes we need to use _pip_ to install extra libraries before we __import__ them. That is what is happening in the next two cell."
      ],
      "metadata": {
        "id": "zj2xRT3osavT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run shell command to see details about GPU connected to this Colab runtime\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NhIa27nZnH4P",
        "outputId": "4af48ce1-a38b-4e73-f818-978efe48e2f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 25 19:52:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRDanDWZn3Hx",
        "outputId": "fbc56b81-b2db-4c28-e90e-f2ee642ba443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-build-core==0.9.0\n",
            "  Downloading scikit_build_core-0.9.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-build-core==0.9.0) (24.2)\n",
            "Collecting pathspec>=0.10.1 (from scikit-build-core==0.9.0)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading scikit_build_core-0.9.0-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, scikit-build-core\n",
            "Successfully installed pathspec-0.12.1 scikit-build-core-0.9.0\n",
            "Using pip 24.1.2 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Collecting llama-cpp-python==0.2.62\n",
            "  Downloading llama_cpp_python-0.2.62.tar.gz (37.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.9.0 using CMake 3.31.4 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Link requires a different Python (3.11.11 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/3a/be/650f9c091ef71cb01d735775d554e068752d3ff63d7943b26316dc401749/numpy-1.21.2.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
            "  Link requires a different Python (3.11.11 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/5f/d6/ad58ded26556eaeaa8c971e08b6466f17c4ac4d786cd3d800e26ce59cc01/numpy-1.21.3.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
            "  Link requires a different Python (3.11.11 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/fb/48/b0708ebd7718a8933f0d3937513ef8ef2f4f04529f1f66ca86d873043921/numpy-1.21.4.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
            "  Link requires a different Python (3.11.11 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/c2/a8/a924a09492bdfee8c2ec3094d0a13f2799800b4fdc9c890738aeeb12c72e/numpy-1.21.5.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
            "  Link requires a different Python (3.11.11 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/45/b7/de7b8e67f2232c26af57c205aaad29fe17754f793404f59c8a730c7a191a/numpy-1.21.6.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/e6/d7/3cd47b00b8ea95ab358c376cf5602ad21871410950bc754cf3284771f8b6/numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m177.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/bd/0f/2ba5fbcd631e3e88689309dbe978c5769e883e4b84ebfe7da30b43275c5a/jinja2-3.1.5-py3-none-any.whl.metadata\n",
            "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/f1/a4/aefb044a2cd8d7334c8a47d3fb2c9f328ac48cb349468cc31c20b539305f/MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m189.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m187.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.9.0 using CMake 3.31.4 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmpf1sba7by/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:397 (message):\n",
            "    LLAMA_CUBLAS is deprecated and will be removed in the future.\n",
            "\n",
            "    Use LLAMA_CUDA instead\n",
            "\n",
            "\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "  -- CUDA found\n",
            "  -- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 61\n",
            "  -- CUDA host compiler is GNU 11.4.0\n",
            "\n",
            "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:26 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:35 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (4.7s)\n",
            "  -- Generating done (0.1s)\n",
            "  -- Build files have been written to: /tmp/tmpf1sba7by/build\n",
            "  *** Building project with Unix Makefiles...\n",
            "  Change Dir: '/tmp/tmpf1sba7by/build'\n",
            "\n",
            "  Run Build Command(s): /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E env VERBOSE=1 /usr/bin/gmake -f Makefile\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -S/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb -B/tmp/tmpf1sba7by/build --check-build-system CMakeFiles/Makefile.cmake 0\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /tmp/tmpf1sba7by/build/CMakeFiles /tmp/tmpf1sba7by/build//CMakeFiles/progress.marks\n",
            "  /usr/bin/gmake  -f CMakeFiles/Makefile2 all\n",
            "  gmake[1]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml.dir/build.make vendor/llama.cpp/CMakeFiles/ggml.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp /tmp/tmpf1sba7by/build/vendor/llama.cpp/CMakeFiles/ggml.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml.dir/build.make vendor/llama.cpp/CMakeFiles/ggml.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [  1%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF CMakeFiles/ggml.dir/ggml.c.o.d -o CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml.c\n",
            "  [  3%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-alloc.c\n",
            "  [  5%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF CMakeFiles/ggml.dir/ggml-backend.c.o.d -o CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-backend.c\n",
            "  [  7%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF CMakeFiles/ggml.dir/ggml-quants.c.o.d -o CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-quants.c\n",
            "  [  9%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/acc.cu -o CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n",
            "  [ 11%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/alibi.cu -o CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n",
            "  [ 13%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/arange.cu -o CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n",
            "  [ 15%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/argsort.cu -o CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n",
            "  [ 16%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/binbcast.cu -o CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n",
            "  [ 18%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/clamp.cu -o CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n",
            "  [ 20%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/concat.cu -o CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n",
            "  [ 22%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/convert.cu -o CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n",
            "  [ 24%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/cpy.cu -o CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n",
            "  [ 26%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/diagmask.cu -o CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n",
            "  [ 28%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/dmmv.cu -o CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n",
            "  [ 30%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/getrows.cu -o CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n",
            "  [ 32%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/im2col.cu -o CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n",
            "  [ 33%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/mmq.cu -o CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n",
            "  [ 35%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/mmvq.cu -o CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n",
            "  [ 37%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/norm.cu -o CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n",
            "  [ 39%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/pad.cu -o CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n",
            "  [ 41%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/pool2d.cu -o CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n",
            "  [ 43%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/quantize.cu -o CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n",
            "  [ 45%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/rope.cu -o CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n",
            "  [ 47%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/scale.cu -o CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n",
            "  [ 49%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/softmax.cu -o CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n",
            "  [ 50%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/sumrows.cu -o CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n",
            "  [ 52%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/tsembd.cu -o CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n",
            "  [ 54%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/unary.cu -o CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n",
            "  [ 56%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda/upscale.cu -o CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n",
            "  [ 58%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/ggml-cuda.cu -o CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 58%] Built target ggml\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_static.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_static.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp /tmp/tmpf1sba7by/build/vendor/llama.cpp/CMakeFiles/ggml_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_static.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_static.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 60%] Linking CUDA static library libggml_static.a\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/ggml_static.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libggml_static.a CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" \"CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda.cu.o\"\n",
            "  /usr/bin/ranlib libggml_static.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 60%] Built target ggml_static\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_shared.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp /tmp/tmpf1sba7by/build/vendor/llama.cpp/CMakeFiles/ggml_shared.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 62%] Linking CUDA shared library libggml_shared.so\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_shared.dir/link.txt --verbose=1\n",
            "  /usr/bin/g++ -fPIC -shared -Wl,-soname,libggml_shared.so -o libggml_shared.so @CMakeFiles/ggml_shared.dir/objects1.rsp @CMakeFiles/ggml_shared.dir/linkLibs.rsp -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 62%] Built target ggml_shared\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/llama.dir/build.make vendor/llama.cpp/CMakeFiles/llama.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp /tmp/tmpf1sba7by/build/vendor/llama.cpp/CMakeFiles/llama.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/llama.dir/build.make vendor/llama.cpp/CMakeFiles/llama.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 64%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF CMakeFiles/llama.dir/llama.cpp.o.d -o CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/llama.cpp\n",
            "  [ 66%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF CMakeFiles/llama.dir/unicode.cpp.o.d -o CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/unicode.cpp\n",
            "  [ 67%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF CMakeFiles/llama.dir/unicode-data.cpp.o.d -o CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/unicode-data.cpp\n",
            "  [ 69%] Linking CXX shared library libllama.so\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llama.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -fPIC -O3 -DNDEBUG -shared -Wl,-soname,libllama.so -o libllama.so CMakeFiles/llama.dir/llama.cpp.o CMakeFiles/llama.dir/unicode.cpp.o \"CMakeFiles/llama.dir/unicode-data.cpp.o\" CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" \"CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda.cu.o\"   -L/usr/local/cuda/targets/x86_64-linux/lib  -Wl,-rpath,/usr/local/cuda-12.5/targets/x86_64-linux/lib: /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libculibos.a -ldl /usr/lib/x86_64-linux-gnu/librt.a -lcudadevrt -lcudart_static -lrt -lpthread -ldl\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 69%] Built target llama\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/build_info.dir/build.make vendor/llama.cpp/common/CMakeFiles/build_info.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp/common /tmp/tmpf1sba7by/build/vendor/llama.cpp/common/CMakeFiles/build_info.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/build_info.dir/build.make vendor/llama.cpp/common/CMakeFiles/build_info.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 71%] Building CXX object vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF CMakeFiles/build_info.dir/build-info.cpp.o.d -o CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/build-info.cpp\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 73%] Built target build_info\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/common.dir/build.make vendor/llama.cpp/common/CMakeFiles/common.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp/common /tmp/tmpf1sba7by/build/vendor/llama.cpp/common/CMakeFiles/common.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/common.dir/build.make vendor/llama.cpp/common/CMakeFiles/common.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 75%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF CMakeFiles/common.dir/common.cpp.o.d -o CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/common.cpp\n",
            "  [ 77%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF CMakeFiles/common.dir/sampling.cpp.o.d -o CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/sampling.cpp\n",
            "  [ 79%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF CMakeFiles/common.dir/console.cpp.o.d -o CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/console.cpp\n",
            "  [ 81%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF CMakeFiles/common.dir/grammar-parser.cpp.o.d -o CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [ 83%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n",
            "  [ 84%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF CMakeFiles/common.dir/train.cpp.o.d -o CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/train.cpp\n",
            "  [ 86%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF CMakeFiles/common.dir/ngram-cache.cpp.o.d -o CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/ngram-cache.cpp\n",
            "  [ 88%] Linking CXX static library libcommon.a\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/common.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/common && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/common.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libcommon.a CMakeFiles/common.dir/common.cpp.o CMakeFiles/common.dir/sampling.cpp.o CMakeFiles/common.dir/console.cpp.o \"CMakeFiles/common.dir/grammar-parser.cpp.o\" \"CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\" CMakeFiles/common.dir/train.cpp.o \"CMakeFiles/common.dir/ngram-cache.cpp.o\" \"CMakeFiles/build_info.dir/build-info.cpp.o\"\n",
            "  /usr/bin/ranlib libcommon.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 88%] Built target common\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 90%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF CMakeFiles/llava.dir/llava.cpp.o.d -o CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/llava.cpp\n",
            "  [ 92%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF CMakeFiles/llava.dir/clip.cpp.o.d -o CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/clip.cpp\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 92%] Built target llava\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 94%] Linking CXX static library libllava_static.a\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -P CMakeFiles/llava_static.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libllava_static.a CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o\n",
            "  /usr/bin/ranlib libllava_static.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 94%] Built target llava_static\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 96%] Linking CXX shared library libllava.so\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava_shared.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -fPIC -O3 -DNDEBUG -shared -Wl,-soname,libllava.so -o libllava.so CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o ../../CMakeFiles/ggml.dir/ggml.c.o \"../../CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"../../CMakeFiles/ggml.dir/ggml-backend.c.o\" \"../../CMakeFiles/ggml.dir/ggml-quants.c.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda.cu.o\"  -Wl,-rpath,/tmp/tmpf1sba7by/build/vendor/llama.cpp:/usr/local/cuda-12.5/targets/x86_64-linux/lib: ../../libllama.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libculibos.a /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so -ldl /usr/lib/x86_64-linux-gnu/librt.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 96%] Built target llava_shared\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  cd /tmp/tmpf1sba7by/build && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpf1sba7by/build'\n",
            "  [ 98%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/common/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
            "  [100%] Linking CXX executable llava-cli\n",
            "  cd /tmp/tmpf1sba7by/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava-cli.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -O3 -DNDEBUG \"CMakeFiles/llava-cli.dir/llava-cli.cpp.o\" CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o -o llava-cli  -Wl,-rpath,/tmp/tmpf1sba7by/build/vendor/llama.cpp:/usr/local/cuda-12.5/targets/x86_64-linux/lib: ../../common/libcommon.a ../../libllama.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.5/targets/x86_64-linux/lib/libculibos.a /usr/local/cuda-12.5/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so -ldl /usr/lib/x86_64-linux-gnu/librt.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  [100%] Built target llava-cli\n",
            "  gmake[1]: Leaving directory '/tmp/tmpf1sba7by/build'\n",
            "  /usr/local/lib/python3.11/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /tmp/tmpf1sba7by/build/CMakeFiles 0\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/include/ggml-alloc.h\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/include/ggml-backend.h\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/lib/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpf1sba7by/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpf1sba7by/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/lib/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpf1sba7by/wheel/platlib/lib/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/bin/llava-cli\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpf1sba7by/wheel/platlib/bin/llava-cli\" to \"\"\n",
            "  -- Installing: /tmp/tmpf1sba7by/wheel/platlib/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpf1sba7by/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-p71cd6q5/llama-cpp-python_8d70e79da7bb4b2fa5ce418d9a5e84cb/llama_cpp/libllava.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.62-cp311-cp311-linux_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.62-cp311-cp311-linux_x86_64.whl size=18466368 sha256=83971bf9679fdedc0cf998c6f5b3eb0df8b67c3dc3633dbab1bded32e445b151\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-asy4x4qx/wheels/65/72/75/fa9568e58203f2d995ccd5e97e0591732ec7da90ec6fa19573\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/__pycache__/typing_extensions.cpython-311.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/typing_extensions-4.12.2.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/numpy-1.26.4.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  changing mode of /usr/local/bin/numpy-config to 755\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/MarkupSafe-3.0.2.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/markupsafe/\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.5\n",
            "    Uninstalling Jinja2-3.1.5:\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/jinja2-3.1.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/jinja2/\n",
            "      Successfully uninstalled Jinja2-3.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.3 which is incompatible.\n",
            "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.3 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.3 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.5 llama-cpp-python-0.2.62 numpy-2.2.3 typing-extensions-4.12.2\n"
          ]
        }
      ],
      "source": [
        "# We will need to install llama-cpp and huggingface_hub before we begin.\n",
        "# These library are not in the corelibraries in Colab\n",
        "# This will take 4 minutes or so to run\n",
        "# prop llama details from: https://github.com/abetlen/llama-cpp-python/issues/1366\n",
        "\n",
        "\n",
        "!pip install scikit-build-core==0.9.0\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.62 --force-reinstall --upgrade --no-cache-dir --verbose --no-build-isolation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Import our Libraries\n",
        "\n",
        "Now that we have the extra parts installed lets pull in those libaries along with everything else we need"
      ],
      "metadata": {
        "id": "aT7G1sjQvf5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "V-XUeWT4vp0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title What Model to Use {\"run\":\"auto\",\"vertical-output\":true}\n",
        "model_name = \"Qwen/Qwen2-7B-Instruct-GGUF\" # @param {\"type\":\"string\"}\n",
        "model_basename = \"qwen2-7b-instruct-q4_k_m.gguf\" # @param {\"type\":\"string\"}\n",
        "\n",
        "print(\"Model choice updated!\")\n",
        "print(\" \")\n",
        "print(\"> \",model_name)\n",
        "print(\"> \",model_basename)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bfeyYD7hYVPg",
        "outputId": "b5711942-9668-43c3-eeb3-a4f816fec823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model choice updated!\n",
            " \n",
            ">  Qwen/Qwen2-7B-Instruct-GGUF\n",
            ">  qwen2-7b-instruct-q4_k_m.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this cell will take about 2-3 minutes to run, depending\n",
        "#on how large your model is, and how good bandwidth is\n",
        "model_path = hf_hub_download(repo_id=model_name, filename=model_basename)\n",
        "llm = Llama(model_path=model_path)\n",
        "print(\"Reading!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d_nYgoDsYWM",
        "outputId": "95d47adc-cd90-41c4-dded-0030d6e1bba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct-GGUF/snapshots/c3024c6fff0a02d52119ecee024bbb93d4b4b8e4/qwen2-7b-instruct-q4_k_m.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.name str              = qwen2-7b-instruct\n",
            "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 18944\n",
            "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 28\n",
            "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = ../Qwen2/gguf/qwen2-7b-imatrix/imatri...\n",
            "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = ../sft_2406.txt\n",
            "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 1937\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 421/152064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = qwen2\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 152064\n",
            "llm_load_print_meta: n_merges         = 151387\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 3584\n",
            "llm_load_print_meta: n_head           = 28\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 7\n",
            "llm_load_print_meta: n_embd_k_gqa     = 512\n",
            "llm_load_print_meta: n_embd_v_gqa     = 512\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 18944\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.62 B\n",
            "llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) \n",
            "llm_load_print_meta: general.name     = qwen2-7b-instruct\n",
            "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
            "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
            "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/29 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  4460.45 MiB\n",
            "...................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:  CUDA_Host KV buffer size =    28.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   28.00 MiB, K (f16):   14.00 MiB, V (f16):   14.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   744.25 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 986\n",
            "llama_new_context_with_model: graph splits = 396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '196', 'quantize.imatrix.dataset': '../sft_2406.txt', 'quantize.imatrix.chunks_count': '1937', 'quantize.imatrix.file': '../Qwen2/gguf/qwen2-7b-imatrix/imatrix.dat', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.architecture': 'qwen2', 'qwen2.block_count': '28', 'qwen2.context_length': '32768', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '4', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '3584', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.file_type': '15', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '18944', 'tokenizer.ggml.model': 'gpt2', 'general.name': 'qwen2-7b-instruct', 'tokenizer.ggml.pre': 'qwen2'}\n",
            "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=model_path)"
      ],
      "metadata": {
        "id": "sFpyphkXst_H",
        "outputId": "656be5b4-39c8-484d-b5a0-e1ca1b564ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0c57fa1f64d5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask a question\n",
        "\n",
        "To start with we'll just ask a simple question"
      ],
      "metadata": {
        "id": "f-g2eag5wxAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a python script that will load a CSV file into a pandas dataframe\"\n",
        "\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "As6ISP9twnCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
        "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
        "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
        "      echo=True # Echo the prompt back in the output\n",
        ") # Generate a completion, can also call create_completion\n",
        "print(output)"
      ],
      "metadata": {
        "id": "0PrKmdU0xLwr",
        "outputId": "1d649d32-b5ee-40b2-9772-27204cb3cb63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    5469.80 ms\n",
            "llama_print_timings:      sample time =      20.69 ms /     9 runs   (    2.30 ms per token,   435.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5469.64 ms /    13 tokens (  420.74 ms per token,     2.38 tokens per second)\n",
            "llama_print_timings:        eval time =    4709.80 ms /     8 runs   (  588.73 ms per token,     1.70 tokens per second)\n",
            "llama_print_timings:       total time =   10359.57 ms /    21 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-53d98e1c-67aa-4023-9786-ed2246abe4c3', 'object': 'text_completion', 'created': 1740513716, 'model': '/root/.cache/huggingface/hub/models--Qwen--Qwen2-7B-Instruct-GGUF/snapshots/c3024c6fff0a02d52119ecee024bbb93d4b4b8e4/qwen2-7b-instruct-q4_k_m.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A:  The answer to your question is as follows:', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 9, 'total_tokens': 22}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All things told\n",
        "\n",
        "I hope this introduction to LLMs has done a few things for you: opened your eyes to the fact there are many, many, different LLMs out there.\n",
        "\n",
        "As a next step I would encourage you to install something like [Anaconda]() which is a Jupyter Notebook environment that you can run on your local computer. Then you can really leverage the horsepower you have at hand on not rely on a free version of Colab. If that doesn't work, you might want to try [Colab Pro](https://colab.research.google.com/signup?utm_source=footer&utm_medium=link&utm_campaign=footer_links). In Canada, if you are afliated with DRAC you can use a hosted version of Jupyter Notebooks called [Syzygy](https://syzygy.ca/)\n"
      ],
      "metadata": {
        "id": "fNYzyu9Kp2v9"
      }
    }
  ]
}